{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Image Recognition using MLP , KNN and CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Source Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: David J. Slate Odesta Corporation; 1890 Maple Ave; Suite 115; Evanston, IL 60201  \n",
    "Donor: David J. Slate (dave@math.nwu.edu) (708) 491-3867  \n",
    "Date: January, 1991  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Relevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character images is based on 20 different fonts and each letter within these 20 fonts has been randomly distorted to produce a file of 20,000 unique stimuli.  Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15. We typically train on the first 16000 items and then use the resulting model to predict the letter category for the remaining 4000 (**NOT in this assignment**). See the article cited below for more details: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P. W. Frey and D. J. Slate (Machine Learning Vol 6 #2 March 91): \"Letter Recognition Using Holland-style Adaptive Classifiers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header line: ['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
      "['T', '2', '8', '3', '5', '1', '8', '13', '0', '6', '6', '10', '8', '0', '8', '0', '8']\n",
      "Total number of rows: 20000\n"
     ]
    }
   ],
   "source": [
    "with open('Letter.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(\"Header line: %s\" % next(reader))\n",
    "    annotated_data = [r for r in reader]\n",
    "print(annotated_data[0])\n",
    "print(\"Total number of rows:\", len(annotated_data))\n",
    "\n",
    "df = pd.DataFrame(annotated_data,columns=['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analaysis(EDA)\n",
    "- Class Distribution \n",
    "- Data Separation\n",
    "- Data Balance\n",
    "- Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "- Compute and print the percentage and its number of stimuli corresponding to the five letters A-E (class label lettr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: is 3.945 %  with Totoal of:  789\n",
      "B: is 3.83 % with Totoal of:  766\n",
      "C: is 3.6799999999999997 % with Totoal of:  736\n",
      "D: is 4.025 % with Totoal of:  805\n",
      "E: is 3.84 % with Totoal of:  768\n"
     ]
    }
   ],
   "source": [
    "df2 = df[df['lettr'] == 'A']\n",
    "print('A: is',df2['lettr'].count()/df['lettr'].count()*100,'%', ' with Totoal of: ', df2['lettr'].count() )\n",
    "df2 = df[df['lettr'] == 'B']\n",
    "print('B: is',df2['lettr'].count()/df['lettr'].count()*100,'%', 'with Totoal of: ', df2['lettr'].count() )\n",
    "df2 = df[df['lettr'] == 'C']\n",
    "print('C: is',df2['lettr'].count()/df['lettr'].count()*100,'%', 'with Totoal of: ', df2['lettr'].count() )\n",
    "df2 = df[df['lettr'] == 'D']\n",
    "print('D: is',df2['lettr'].count()/df['lettr'].count()*100,'%', 'with Totoal of: ', df2['lettr'].count() )\n",
    "df2 = df[df['lettr'] == 'E']\n",
    "print('E: is',df2['lettr'].count()/df['lettr'].count()*100,'%', 'with Totoal of: ', df2['lettr'].count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data for training and testing purpose.\n",
    "Split the data into a training set, a dev-test set, and a test set. Use the following ratio for splitting the data:\n",
    "\n",
    "* Training set: 80%\n",
    "* Dev-test set: 10%\n",
    "* Test set: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "random.seed(1234)  \n",
    "random.shuffle(annotated_data)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above three lines of code are used to randomize the order of the data.  \n",
    "After that,  use the first 80% as training set, then 10% as Dev-test, and the last 10% as test set.\n",
    "- At this point annotated_data is shuffled, therefore i create the dataframe of annotated_data to have a randomised dataset Then I proceed the split\n",
    "- I simply take the first 80% as train next 10% as Dev_test and last 10% as Test\n",
    "- splitting datasets into x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (16000, 17)\n",
      "Dev-Test set: (2000, 17)\n",
      "Test set: (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(annotated_data,columns=['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar',\n",
    "                                          'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx'])\n",
    "\n",
    "\n",
    "train = df.iloc[:int(len(df)*0.8)]\n",
    "Dev_test = df.iloc[int(len(df)*0.8):int(len(df)*0.9)]\n",
    "test = df.iloc[int(len(df)*0.9):int(len(df))]\n",
    "\n",
    "print('Training set:', train.shape)\n",
    "print('Dev-Test set:',Dev_test.shape)\n",
    "print('Test set:',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set\n",
    "x_train = train.drop(columns=['lettr'])\n",
    "y_train = train['lettr']\n",
    "\n",
    "#Validation set\n",
    "x_valid = Dev_test.drop(columns=['lettr'])\n",
    "y_valid = Dev_test['lettr']\n",
    "\n",
    "#Test set\n",
    "x_test = test.drop(columns=['lettr'])\n",
    "y_test = test['lettr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "- Use one hot encoder methods to encode labels\n",
    "- Encoded labels are only used in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "\n",
    "#Encode Y_train\n",
    "train_integer_encoded = label_encoder.fit_transform(train['lettr'])\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "train_integer_encoded = train_integer_encoded.reshape(len(train_integer_encoded), 1)\n",
    "y_encoded_train = onehot_encoder.fit_transform(train_integer_encoded)\n",
    "\n",
    "#Encode Y_test\n",
    "test_integer_encoded = label_encoder.fit_transform(test['lettr']) \n",
    "test_integer_encoded = test_integer_encoded.reshape(len(test_integer_encoded), 1)\n",
    "y_encoded_test = onehot_encoder.fit_transform(test_integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Encoded_labels\n",
    "y_encoded_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the data are balanced after splitting\n",
    "- Print the percentage of class label lettr (A-E) in each partition, and check that they are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  3.9375 % totoal of:  630\n",
      "B:  3.7562499999999996 % totoal of:  601\n",
      "C:  3.6062499999999997 % totoal of:  577\n",
      "D:  4.075 % totoal of:  652\n",
      "E:  3.88125 % totoal of:  621\n"
     ]
    }
   ],
   "source": [
    "df2 = train[train['lettr'] == 'A']\n",
    "print('A: ',df2['lettr'].count()/train['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = train[train['lettr'] == 'B']\n",
    "print('B: ',df2['lettr'].count()/train['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = train[train['lettr'] == 'C']\n",
    "print('C: ',df2['lettr'].count()/train['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = train[train['lettr'] == 'D']\n",
    "print('D: ',df2['lettr'].count()/train['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = train[train['lettr'] == 'E']\n",
    "print('E: ',df2['lettr'].count()/train['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:  4.2 % totoal of:  84\n",
      "B:  3.75 % totoal of:  75\n",
      "C:  4.15 % totoal of:  83\n",
      "D:  4.2 % totoal of:  84\n",
      "E:  3.15 % totoal of:  63\n"
     ]
    }
   ],
   "source": [
    "df2 = test[test['lettr'] == 'A']\n",
    "print('A: ',df2['lettr'].count()/test['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = test[test['lettr'] == 'B']\n",
    "print('B: ',df2['lettr'].count()/test['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = test[test['lettr'] == 'C']\n",
    "print('C: ',df2['lettr'].count()/test['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = test[test['lettr'] == 'D']\n",
    "print('D: ',df2['lettr'].count()/test['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n",
    "\n",
    "df2 = test[test['lettr'] == 'E']\n",
    "print('E: ',df2['lettr'].count()/test['lettr'].count()*100,'%', 'totoal of: ', df2['lettr'].count() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "- Looking at the percentage of each class from different dataset, the differences are at most + - 0.5% between different dataset\n",
    "- Within the datasets each label is +- 0.5% from the average of 3.85%\n",
    "- Therefore, we can safe to assume every label is evenly distributed in Train set, Validation set and Test set \n",
    "- The datasets are balanced \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "- caculate mean and standard diviation for X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Compute the mean value per feature  (except for the class lable) in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_mean contains the mean of each features \n",
    "x_mean = x_train.astype(int).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the standard deviation of each feature  (except for the class lable) in the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_std contains the standard deviation of each features \n",
    "x_std = x_train.astype(int).std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Scaling the training set] subtract the mean, and scale by inverse standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-box</th>\n",
       "      <th>y-box</th>\n",
       "      <th>width</th>\n",
       "      <th>high</th>\n",
       "      <th>onpix</th>\n",
       "      <th>x-bar</th>\n",
       "      <th>y-bar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x-ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y-ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515738</td>\n",
       "      <td>1.200393</td>\n",
       "      <td>1.429765</td>\n",
       "      <td>1.164333</td>\n",
       "      <td>0.226582</td>\n",
       "      <td>-0.946731</td>\n",
       "      <td>0.218675</td>\n",
       "      <td>-0.973193</td>\n",
       "      <td>1.181470</td>\n",
       "      <td>0.691783</td>\n",
       "      <td>1.728710</td>\n",
       "      <td>0.516272</td>\n",
       "      <td>-0.020188</td>\n",
       "      <td>0.427530</td>\n",
       "      <td>0.120829</td>\n",
       "      <td>-1.120510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.531180</td>\n",
       "      <td>-0.312938</td>\n",
       "      <td>-0.057250</td>\n",
       "      <td>-0.606642</td>\n",
       "      <td>-1.143701</td>\n",
       "      <td>0.543206</td>\n",
       "      <td>1.084499</td>\n",
       "      <td>-0.973193</td>\n",
       "      <td>-1.332964</td>\n",
       "      <td>-0.514640</td>\n",
       "      <td>2.487998</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>-0.448009</td>\n",
       "      <td>1.719285</td>\n",
       "      <td>-1.434805</td>\n",
       "      <td>0.119507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007721</td>\n",
       "      <td>1.200393</td>\n",
       "      <td>0.438422</td>\n",
       "      <td>1.164333</td>\n",
       "      <td>1.140104</td>\n",
       "      <td>-0.450085</td>\n",
       "      <td>-0.647150</td>\n",
       "      <td>0.505332</td>\n",
       "      <td>-0.075747</td>\n",
       "      <td>-0.514640</td>\n",
       "      <td>0.210133</td>\n",
       "      <td>0.997476</td>\n",
       "      <td>2.118919</td>\n",
       "      <td>-2.155979</td>\n",
       "      <td>-0.656988</td>\n",
       "      <td>0.739515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.007721</td>\n",
       "      <td>-0.615604</td>\n",
       "      <td>-0.552922</td>\n",
       "      <td>-1.049386</td>\n",
       "      <td>-0.230179</td>\n",
       "      <td>0.046561</td>\n",
       "      <td>0.218675</td>\n",
       "      <td>0.135701</td>\n",
       "      <td>-0.075747</td>\n",
       "      <td>-0.514640</td>\n",
       "      <td>-0.169511</td>\n",
       "      <td>-0.927340</td>\n",
       "      <td>0.835455</td>\n",
       "      <td>1.073408</td>\n",
       "      <td>-0.656988</td>\n",
       "      <td>-1.740519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515738</td>\n",
       "      <td>-0.615604</td>\n",
       "      <td>0.934093</td>\n",
       "      <td>0.721589</td>\n",
       "      <td>0.226582</td>\n",
       "      <td>1.039852</td>\n",
       "      <td>0.218675</td>\n",
       "      <td>0.135701</td>\n",
       "      <td>-1.332964</td>\n",
       "      <td>-0.514640</td>\n",
       "      <td>0.589777</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>2.546740</td>\n",
       "      <td>0.427530</td>\n",
       "      <td>-1.434805</td>\n",
       "      <td>0.119507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x-box     y-box     width      high     onpix     x-bar     y-bar  \\\n",
       "0  0.515738  1.200393  1.429765  1.164333  0.226582 -0.946731  0.218675   \n",
       "1 -0.531180 -0.312938 -0.057250 -0.606642 -1.143701  0.543206  1.084499   \n",
       "2 -0.007721  1.200393  0.438422  1.164333  1.140104 -0.450085 -0.647150   \n",
       "3 -0.007721 -0.615604 -0.552922 -1.049386 -0.230179  0.046561  0.218675   \n",
       "4  0.515738 -0.615604  0.934093  0.721589  0.226582  1.039852  0.218675   \n",
       "\n",
       "      x2bar     y2bar     xybar     x2ybr     xy2br     x-ege     xegvy  \\\n",
       "0 -0.973193  1.181470  0.691783  1.728710  0.516272 -0.020188  0.427530   \n",
       "1 -0.973193 -1.332964 -0.514640  2.487998  0.035068 -0.448009  1.719285   \n",
       "2  0.505332 -0.075747 -0.514640  0.210133  0.997476  2.118919 -2.155979   \n",
       "3  0.135701 -0.075747 -0.514640 -0.169511 -0.927340  0.835455  1.073408   \n",
       "4  0.135701 -1.332964 -0.514640  0.589777  0.035068  2.546740  0.427530   \n",
       "\n",
       "      y-ege     yegvx  \n",
       "0  0.120829 -1.120510  \n",
       "1 -1.434805  0.119507  \n",
       "2 -0.656988  0.739515  \n",
       "3 -0.656988 -1.740519  \n",
       "4 -1.434805  0.119507  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I created a new datafram called x_Scale_train to store all Standardization features value\n",
    "\n",
    "x_Scale_train = ((x_train.astype(int)- x_mean )/ x_std)\n",
    "x_Scale_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same (using training mean and std) with respect to the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-box</th>\n",
       "      <th>y-box</th>\n",
       "      <th>width</th>\n",
       "      <th>high</th>\n",
       "      <th>onpix</th>\n",
       "      <th>x-bar</th>\n",
       "      <th>y-bar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x-ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y-ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>-1.067475</td>\n",
       "      <td>0.872716</td>\n",
       "      <td>-1.089248</td>\n",
       "      <td>1.116359</td>\n",
       "      <td>-1.141729</td>\n",
       "      <td>2.444594</td>\n",
       "      <td>-1.870841</td>\n",
       "      <td>2.000418</td>\n",
       "      <td>-0.906899</td>\n",
       "      <td>1.890593</td>\n",
       "      <td>0.188712</td>\n",
       "      <td>2.394550</td>\n",
       "      <td>-0.911053</td>\n",
       "      <td>-1.498661</td>\n",
       "      <td>-1.439239</td>\n",
       "      <td>0.128585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18001</th>\n",
       "      <td>1.466289</td>\n",
       "      <td>0.872716</td>\n",
       "      <td>1.868655</td>\n",
       "      <td>1.116359</td>\n",
       "      <td>1.515515</td>\n",
       "      <td>-1.378070</td>\n",
       "      <td>0.645985</td>\n",
       "      <td>-0.233230</td>\n",
       "      <td>1.229998</td>\n",
       "      <td>1.491565</td>\n",
       "      <td>1.314232</td>\n",
       "      <td>0.481015</td>\n",
       "      <td>-0.055403</td>\n",
       "      <td>-0.223476</td>\n",
       "      <td>0.521844</td>\n",
       "      <td>-2.320645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18002</th>\n",
       "      <td>-0.560722</td>\n",
       "      <td>-0.329579</td>\n",
       "      <td>-0.103280</td>\n",
       "      <td>-0.633077</td>\n",
       "      <td>-0.255981</td>\n",
       "      <td>0.055429</td>\n",
       "      <td>1.065456</td>\n",
       "      <td>-0.977780</td>\n",
       "      <td>-1.334279</td>\n",
       "      <td>-0.503573</td>\n",
       "      <td>0.939059</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>1.228071</td>\n",
       "      <td>1.689301</td>\n",
       "      <td>-1.439239</td>\n",
       "      <td>0.128585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18003</th>\n",
       "      <td>0.959536</td>\n",
       "      <td>0.572142</td>\n",
       "      <td>-0.103280</td>\n",
       "      <td>-0.633077</td>\n",
       "      <td>-0.698855</td>\n",
       "      <td>1.966761</td>\n",
       "      <td>-2.290312</td>\n",
       "      <td>-0.977780</td>\n",
       "      <td>-0.052140</td>\n",
       "      <td>0.693510</td>\n",
       "      <td>-1.687154</td>\n",
       "      <td>0.481015</td>\n",
       "      <td>-0.483228</td>\n",
       "      <td>-0.223476</td>\n",
       "      <td>-0.262589</td>\n",
       "      <td>1.965507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18004</th>\n",
       "      <td>-0.560722</td>\n",
       "      <td>-0.029005</td>\n",
       "      <td>-0.103280</td>\n",
       "      <td>-0.195718</td>\n",
       "      <td>-0.255981</td>\n",
       "      <td>-1.378070</td>\n",
       "      <td>0.226514</td>\n",
       "      <td>0.511319</td>\n",
       "      <td>0.375239</td>\n",
       "      <td>-0.503573</td>\n",
       "      <td>0.939059</td>\n",
       "      <td>0.959399</td>\n",
       "      <td>-0.055403</td>\n",
       "      <td>0.414116</td>\n",
       "      <td>-1.047022</td>\n",
       "      <td>0.128585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x-box     y-box     width      high     onpix     x-bar     y-bar  \\\n",
       "18000 -1.067475  0.872716 -1.089248  1.116359 -1.141729  2.444594 -1.870841   \n",
       "18001  1.466289  0.872716  1.868655  1.116359  1.515515 -1.378070  0.645985   \n",
       "18002 -0.560722 -0.329579 -0.103280 -0.633077 -0.255981  0.055429  1.065456   \n",
       "18003  0.959536  0.572142 -0.103280 -0.633077 -0.698855  1.966761 -2.290312   \n",
       "18004 -0.560722 -0.029005 -0.103280 -0.195718 -0.255981 -1.378070  0.226514   \n",
       "\n",
       "          x2bar     y2bar     xybar     x2ybr     xy2br     x-ege     xegvy  \\\n",
       "18000  2.000418 -0.906899  1.890593  0.188712  2.394550 -0.911053 -1.498661   \n",
       "18001 -0.233230  1.229998  1.491565  1.314232  0.481015 -0.055403 -0.223476   \n",
       "18002 -0.977780 -1.334279 -0.503573  0.939059  0.002631  1.228071  1.689301   \n",
       "18003 -0.977780 -0.052140  0.693510 -1.687154  0.481015 -0.483228 -0.223476   \n",
       "18004  0.511319  0.375239 -0.503573  0.939059  0.959399 -0.055403  0.414116   \n",
       "\n",
       "          y-ege     yegvx  \n",
       "18000 -1.439239  0.128585  \n",
       "18001  0.521844 -2.320645  \n",
       "18002 -1.439239  0.128585  \n",
       "18003 -0.262589  1.965507  \n",
       "18004 -1.047022  0.128585  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_Scale_test=((x_test.astype(int)- x_test.astype(int).mean()) / x_test.astype(int).std())\n",
    "x_Scale_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP) classifier\n",
    "- Default State \n",
    "- random_stae = 0\n",
    "- Max_iter = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 18.253190279006958 seconds ---\n",
      "Test set accuracy: 0.921\n",
      "Training set accuracy: 0.9540625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import  accuracy_score\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "clf = MLPClassifier(random_state=0,max_iter=300)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "predicted = clf.predict(x_test)\n",
    "\n",
    "MLP_test_accuracy = accuracy_score(y_test, predicted)\n",
    "MLP_train_accuracy = accuracy_score(y_train, clf.predict(x_train))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Test set accuracy:', accuracy_score(y_test, predicted))\n",
    "print('Training set accuracy:',accuracy_score(y_train, clf.predict(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 18.454651832580566 seconds ---\n",
      "Test set accuracy: 0.949\n",
      "Training set accuracy: 0.9944375\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf = MLPClassifier(random_state=0,max_iter=300)\n",
    "clf.fit(x_Scale_train,y_train)\n",
    "predicted = clf.predict(x_Scale_test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('Test set accuracy:', accuracy_score(y_test, predicted))\n",
    "print('Training set accuracy:',accuracy_score(y_train, clf.predict(x_Scale_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neuron Network\n",
    "- Using Tesorflow\n",
    "- Sequential model \n",
    "- 4 Convolution layers\n",
    "- 2 Dense layers \n",
    "- batch_size = 50 \n",
    "- epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hank\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping\n",
    "x_train = x_train.values.reshape(-1,4,4,1)\n",
    "x_test = x_test.values.reshape(-1,4,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 2s 98us/step - loss: 1.6633 - acc: 0.5355 - val_loss: 1.0378 - val_acc: 0.7090\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 1s 70us/step - loss: 0.7977 - acc: 0.7736 - val_loss: 0.7408 - val_acc: 0.7825\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.6115 - acc: 0.8192 - val_loss: 0.5742 - val_acc: 0.8330\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.4855 - acc: 0.8526 - val_loss: 0.4933 - val_acc: 0.8530\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 1s 70us/step - loss: 0.3943 - acc: 0.8754 - val_loss: 0.3976 - val_acc: 0.8700\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 1s 70us/step - loss: 0.3267 - acc: 0.8961 - val_loss: 0.3602 - val_acc: 0.8920\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.2850 - acc: 0.9071 - val_loss: 0.3969 - val_acc: 0.8755\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.2588 - acc: 0.9162 - val_loss: 0.3358 - val_acc: 0.8960\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.2321 - acc: 0.9251 - val_loss: 0.3300 - val_acc: 0.8895\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 1s 69us/step - loss: 0.2096 - acc: 0.9301 - val_loss: 0.3110 - val_acc: 0.9045\n",
      "--- 12.04246735572815 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = (1,1), padding = \"same\", activation = 'relu' , input_shape = (4,4,1)))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (1,1), padding = \"same\", activation = 'relu' ))\n",
    "model.add(Conv2D(filters = 5, kernel_size = (2,2), padding = \"same\", activation = 'relu' , input_shape = (4,4,1)))\n",
    "model.add(Conv2D(filters = 5, kernel_size = (2,2), padding = \"same\", activation = 'relu' ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(26, activation = \"softmax\"))\n",
    "model.compile(optimizer = 'adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#timer\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_encoded_train , batch_size = 50, epochs = 10  , validation_data = (x_test, y_encoded_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with sacled datasets\n",
    "- Here we want to see does the scaled makes difference in CNN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Scale_train = x_Scale_train.values.reshape(-1,4,4,1)\n",
    "x_Scale_test = x_Scale_test.values.reshape(-1,4,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.7012 - acc: 0.7863 - val_loss: 0.3804 - val_acc: 0.8785\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.2590 - acc: 0.9182 - val_loss: 0.2832 - val_acc: 0.9140\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.1933 - acc: 0.9361 - val_loss: 0.2993 - val_acc: 0.9055\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.1587 - acc: 0.9473 - val_loss: 0.2994 - val_acc: 0.9120\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.1389 - acc: 0.9533 - val_loss: 0.2647 - val_acc: 0.9215\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.1198 - acc: 0.9594 - val_loss: 0.2574 - val_acc: 0.9280\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.1097 - acc: 0.9621 - val_loss: 0.2107 - val_acc: 0.9390\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.0962 - acc: 0.9666 - val_loss: 0.2051 - val_acc: 0.9370\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.0915 - acc: 0.9694 - val_loss: 0.1963 - val_acc: 0.9420\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 1s 65us/step - loss: 0.0811 - acc: 0.9707 - val_loss: 0.2096 - val_acc: 0.9340\n",
      "--- 10.475039958953857 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#timer\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_Scale_train, y_encoded_train , batch_size = 50, epochs = 10  , validation_data = (x_Scale_test, y_encoded_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Final Results*\n",
    "\n",
    "## MLP vs CNN\n",
    "\n",
    "### Performance \n",
    "- From the above results we can see both MLP and CNN are able to provides high accuracy peformance\n",
    "- MLP with 92% on test set , similarly CNN with 90% \n",
    "\n",
    "### Computation Time\n",
    "\n",
    "- The time difference between MLP and CNN are relatively significant. Considering the size of the experiment, usually ther will be more variables than 16. I believe MLP will take longer computation than CNN as we can see the differences from above\n",
    "\n",
    "- MLP with 16 Seconds \n",
    "- CNN with 12 Seconds\n",
    "\n",
    "## Standardised Vs Unstandardzed \n",
    "\n",
    "- Due to the assignments, the 16 vairables are collected without any correlation, therefore there is a high vairances and bias need to be consider. To eliminate vairances we standardised the dataset.\n",
    "- We can see this as both methods provides better performance with scaled datasets \n",
    "\n",
    "\n",
    "## Potential problem / Overfitting\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
